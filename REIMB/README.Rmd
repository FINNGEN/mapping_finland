---
title: "reimb"
output:
  github_document:
    toc: true
    df_print: kable
#   html_document:
#     df_print: paged
#     toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())

knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(scales)


getOutputFormat <- function() {
  output <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
    )$output
  if (is.list(output)){
    return(names(output)[1])
  } else {
    return(output[1])
  }
}


# Dynamic markdown functions
my_print_line<- function(...){
  cat(str_c(" \n",...," \n"))
}

my_print_table<- function(...){
  if(getOutputFormat() == 'html_document') {
   cat(knitr::knit_print(rmarkdown::paged_table(...)))
  }else{
    cat(knitr::knit_print(knitr::kable(...)))
  }
  
}

# Document variables 
N_TOPS <- 10

# output status
db_statuses <- tibble()

```

```{r load, message=FALSE, warning=FALSE}
col_types = cols( .default = col_character(), 
                  valid_start_date  = col_date(), 
                  valid_end_date  = col_date()
                  )

reimb_source_vocabulary <- read_csv("./1_source_vocabulary/reimb_for_source_vocabulary.csv", col_types = col_types)

col_types = cols( code = col_character(),
                  .default = col_number()
                  )
reimb_freqs <- read_csv("./3_freq_of_source_codes/freq_total.csv", col_types = col_types)

# col_types = cols( .default = col_character(), 
#                   sourceFrequency = col_integer(),
#                   matchScore  = col_integer(),
#                   conceptId  = col_integer()
#                   )
# 
# reimb_after_usagi <- read_csv("./2_mapping_to_standard/USAGI/reimb_after_usagi.csv", col_types = col_types) %>% 
#   mutate(name_en_icd9 = `ADD_INFO:possible_english_name`)# cant get the encoding rigth
```


# Intro
TODO



# Formating source vocabulary to OMOP
The reimb codes were downloaded from [nose](nose) (3.4.2020). Missing codes, translations and validity dates were manually colledted from kela's web page into table [./1_source_vocabulary/Kela_reimbursement_codes.xlsx](./1_source_vocabulary/Kela_reimbursement_codes.xlsx) by [\@CoderMikolaj](github.com/CoderMikolaj). 

Resulting table contains the codes, names, and validity periods. Some codes (e.g 307) have a different names during different periods, and therefore codes are repeated on the table for name and time period. 

TODO: we got the name changes over time only for the top 21 codes. The rest should be checked. (top 21 bcs were the ones with no name at all kela's )

Missing English translations were made using Google-tralator

```{r}
reimb_source_vocabulary %>% count(name_en_source, sort = T)
```

Source table with added translation has been formatted to be similar to OMOP in  [1_source_vocabulary/reimb_for_source_vocabulary.csv](1_source_vocabulary/reimb_for_source_vocabulary.csv). 



# Mapping the source vocabulary to the standard vocabularies
TODO: In short, USAGI , to conditions high level concepts ?? WE NEED AN EXPERT HERE !!


## Progess in number of codes
```{r}
reimb_after_usagi <- reimb_source_vocabulary %>% 
  mutate(mappingStatus="UNCHECKED", sourceFrequency=0) # fake usagu by seting all as uncheked
  

n_codes <- reimb_after_usagi %>% distinct(code) %>% nrow()
n_codes_accepted <- reimb_after_usagi %>% filter(mappingStatus=="APPROVED") %>% distinct(code) %>% nrow()
```


From `r scales::number(n_codes)` codes `r scales::number(n_codes_accepted)` have been approved. 

This makes `r scales::percent(n_codes_accepted/n_codes)` of codes approved. 


```{r}
a <- reimb_after_usagi %>% distinct(code, mappingStatus, sourceFrequency)

n_total_envents <- sum(a$sourceFrequency, na.rm = T)
n_approved_events <- a %>% filter(mappingStatus=="APPROVED") %>% .$sourceFrequency %>% sum(na.rm = T)

max_n_events_unmapped <-  a %>% filter(mappingStatus=="UNCHECKED") %>% .$sourceFrequency %>%  max(na.rm = T)
```

## Progess in number of events
```{r}
# Accepted codes covers `r scales::percent(n_approved_events/n_total_envents)` of the total number of events in the combined databases. 
# 
# Accepted codes covers all codes with more than `r scales::number(max_n_events_unmapped)` events in the combined databases.
# 
# Top`r N_TOPS` of the unchecked events sort by number of events : 
# 
# 
# reimb_after_usagi %>% filter(mappingStatus=="UNCHECKED") %>% arrange(-sourceFrequency) %>% select(code,  sourceFrequency, matchScore, sourceName, name_en_icd9) %>%  head(N_TOPS)
```

```{r}
db_statuses <- bind_rows(
  tibble(
    status = "mapped",
    n_codes = n_codes_accepted,
    per_codes = percent(n_codes_accepted/n_codes),
    n_events = n_approved_events,
    per_events = percent(n_approved_events/n_total_envents), 
    db_name = "source"
  ),
 tibble(
    status = "not_mapped",
    n_codes = n_codes-n_codes_accepted,
    per_codes = percent((n_codes-n_codes_accepted)/n_codes),
    n_events = n_total_envents-n_approved_events,
    per_events = percent((n_total_envents-n_approved_events)/n_total_envents), 
    db_name = "source"
  )
)
```



# Assessing coverage of databases


```{r  results='asis'}
# get database names from freq tables 
db_names <- reimb_freqs %>% names %>% setdiff(c("code", "freq_total")) %>% str_replace("freq_","")

#for each data base
for(db_name in db_names){
#assess_db <- function(i){
 # db_name <- db_names[i]

  # get only preset dtabase info 
  db_freq <- reimb_freqs %>% 
    select(code, str_c("freq_",db_name)) %>% 
    rename(freq = str_c("freq_",db_name)) %>% 
    filter(!is.na(freq))
  
  n_total_envents <- sum(db_freq$freq)
  
  # join with usagi output 
  db_join <- left_join(
    db_freq ,
    reimb_after_usagi %>% 
      select(code, name_en, name_fi, mappingStatus ), 
    by = "code") %>%
  mutate( freq_per=percent(freq/n_total_envents, accuracy = 0.001)) %>% 
    arrange(-freq)
  
  #save not found codes
  db_missing <- db_join %>% filter(is.na(mappingStatus)) %>% select(code, freq, freq_per)
  save_db_missing_path <- str_c("./3_freq_of_source_codes/",db_name,"_not_in_reimb.csv")
  write_csv(db_missing, save_db_missing_path )
  
  # calculate status count
  db_status <- db_join %>% 
    mutate(status = case_when( 
      is.na(mappingStatus) ~ "not_found",  
      mappingStatus == "UNCHECKED" ~ "not_mapped", 
      TRUE ~ "mapped"
    )
  ) %>% 
  mutate(status = factor(status, levels = c("mapped", "not_mapped",  "not_found"))) %>% 
  group_by(status) %>% 
  summarise(n_codes=n(), 
            per_codes= percent(n_codes/nrow(.), accuracy = 0.001),
            n_events = sum(freq), 
            per_events= percent(n_events/sum(.$freq), accuracy = 0.001), 
            .groups = 'drop')
  
  db_statuses <- bind_rows(db_statuses, db_status %>% mutate(db_name = db_name))
  
   # plot markdown 
  my_print_line()
  my_print_line("## Database ", db_name)
  
  my_print_line("**How many codes labeled as reimb in ", db_name," are not in the reimb standard?**")
  
  my_print_line("There are ", scales::number(db_missing %>% nrow())," codes not found in the standard")
  
  my_print_line("Top", N_TOPS," sort by freq:")
  
  my_print_table(db_missing %>% head(N_TOPS))
  
  my_print_line("The full list can be found in [",save_db_missing_path,"](",save_db_missing_path,")")
  
 
  my_print_line("**Status of the reimb codes in", db_name, "**")
  
  my_print_table(db_status)
  
  
}


write_csv(db_statuses, "status_table.csv" )
  

```




















